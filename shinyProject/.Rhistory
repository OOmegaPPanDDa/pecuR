d$title <- as.character(d$title)
class(d$title)
summary(d)
summary(d)
hist(d$donation, br = 100)
hist(d$donation, br = 100)
hist(d$donation, br = 100)
hist(d$donation, br = 100)
hist(d$donation, breaks =  = 100)
hist(d$donation, breaks = 100)
hist(d$donation, breaks = 100)
na.omit(d$donation)
hist(na.omit(d$donation), breaks = 100)
hist(d$donation, breaks = 100)
plot(d$donor, d$donation, pch = '.', cex = 2)
?plot
?cex
??cex
plot(d$donor, d$donation, pch = '.', cex = 2)
abline(lm(d$donation ~d$donor), col = ‘red’)
abline(lm(d$donation ~d$donor), col = 'red')
abline(lm(d$donation ~d$donor), col = 'red')
plot(d$donor, d$donation, pch = '.', cex = 2)
abline(lm(d$donation ~d$donor), col = 'red')
n <- length(unique(d$journalist))
n
b <- boxplot(d$donation ~ d$journalist, col =
heat.colors(n), las = 2, ylim = c(0,2e6))
b <- boxplot(d$donation ~ d$journalist, col =
heat.colors(n), las = 1, ylim = c(0,2e6))
b <- boxplot(d$donation ~ d$journalist, col =
heat.colors(n), las = 5, ylim = c(0,2e6))
b <- boxplot(d$donation ~ d$journalist, col =
heat.colors(n), las = 1, ylim = c(0,2e6))
b <- boxplot(d$donation ~ d$journalist, col =
heat.colors(n), las = 2, ylim = c(0,2e6))
abline(h = mean(d$donation), lty = 2, cex = 2)
b
abline(h = mean(d$donation), lty = 2, cex = 2)
b
abline(h = mean(d$donation), lty = 2, cex = 2)
b <- boxplot(d$donation ~ d$journalist, col =
heat.colors(n), las = 2, ylim = c(0,2e6))
abline(h = mean(d$donation), lty = 2, cex = 2)
text(1:n, (b$stats[3,]+b$stats[4,])/2, b$n, cex =
0.8)
setwd('~/dsr')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
df
source('~/.active-rstudio-document')
head(df, 4)
source('~/.active-rstudio-document')
feature
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
install.packages('MLmetrics')
source('~/.active-rstudio-document')
F1_Score(prediction, testset[,3])
View(df)
df$身價
k = if(df$身價<=50){}
k = if(df$身價<=50){return 1}
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
df
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
group
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
group
source('~/.active-rstudio-document')
View(df$group)
View(df)
View(df)
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
(tuned <- tune.svm(group~., data = trainset, cost=10^(-1:2), gamma=c(.5,1,2)))
df <- read.table('1215data.txt', sep='\t', header=T)
head(df, 4)
feature <- df[, c(8, 10, 11)]
index <- 1:nrow(df)
testindex <- sample(index, trunc(length(index)*30/100))
trainset <- feature[-testindex,]
testset <- feature[testindex,]
View(testset)
str(df)
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
length(df$身價)
length(df$group)
length(df$身高)
source('~/.active-rstudio-document')
(tuned <- tune.svm(group~., data = trainset, cost=10^(-1:2), gamma=c(.5,1,2)))
(tuned <- tune.svm(是否為網紅~., data = trainset, cost=10^(-1:2), gamma=c(.5,1,2)))
model <- svm(是否為網紅~., data = trainset, kernel='linear', cost = 1, gamma = 1)
(tuned <- tune.svm(是否為網紅~., data = trainset, cost=10^(-1:2), gamma=c(.5,1,2)))
df <- read.table('1215data.txt', sep='\t', header=T)
head(df, 4)
feature <- df[, c(8, 10, 11)]
index <- 1:nrow(df)
testindex <- sample(index, trunc(length(index)*30/100))
trainset <- feature[-testindex,]
testset <- feature[testindex,]
(tuned <- tune.svm(是否為網紅~., data = trainset, cost=10^(-1:2), gamma=c(.5,1,2)))
require(e1071)
require(MLmetrics)
require(dplyr)
df <- read.table('1215data.txt', sep='\t', header=T)
head(df, 4)
feature <- df[, c(8, 10, 11)]
index <- 1:nrow(df)
testindex <- sample(index, trunc(length(index)*30/100))
trainset <- feature[-testindex,]
testset <- feature[testindex,]
count = 1
for (i in df$身價){
if(i < 10000){
df$group[count] = 'y'
count = count + 1
}else{
df$group[count] = 'n'
count = count + 1
}
}
(tuned <- tune.svm(是否為網紅~., data = trainset, cost=10^(-1:2), gamma=c(.5,1,2)))
(tuned <- tune.svm(group~., data = trainset, cost=10^(-1:2), gamma=c(.5,1,2)))
source('~/.active-rstudio-document')
require(e1071)
require(MLmetrics)
require(dplyr)
df <- read.table('1215data.txt', sep='\t', header=T)
count = 1
for (i in df$身價){
if(i < 10000){
df$group[count] = 'y'
count = count + 1
}else{
df$group[count] = 'n'
count = count + 1
}
}
head(df, 4)
feature <- df[, c(8, 10, 11)]
index <- 1:nrow(df)
testindex <- sample(index, trunc(length(index)*30/100))
trainset <- feature[-testindex,]
testset <- feature[testindex,]
(tuned <- tune.svm(是否為網紅~., data = trainset, cost=10^(-1:2), gamma=c(.5,1,2)))
model <- svm(是否為網紅~., data = trainset, kernel='linear', cost = 1, gamma = 1)
prediction <- predict(model, testset[,-3])
model <- svm(group~., data = trainset, kernel='linear', cost = 1, gamma = 1)
model <- svm(group~., data = trainset, kernel='linear', cost = 1, gamma = 1)
require(e1071)
require(MLmetrics)
require(dplyr)
df <- read.table('1215data.txt', sep='\t', header=T)
count = 1
for (i in df$身價){
if(i < 10000){
df$thegroup[count] = 'y'
count = count + 1
}else{
df$thegroup[count] = 'n'
count = count + 1
}
}
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
View(trainset)
View(testset)
testindex
head(df, 4)
feature <- df[, c(6, 7, 13)]
index <- 1:nrow(df)
testindex <- sample(index, trunc(length(index)*30/100))
trainset <- feature[-testindex,]
testset <- feature[testindex,]
(tuned <- tune.svm(thegroup~., data = trainset, cost=10^(-1:2), gamma=c(.5,1,2)))
model <- svm(thegroup~., data = trainset, kernel='linear', cost = 1, gamma = 1)
prediction <- predict(model, testset[,-3])
prediction
ConfusionMatrix(prediction, testset[,3])
F1_Score(prediction, testset[,3])
source('~/.active-rstudio-document')
(tuned <- tune.svm(thegroup~., data = trainset, cost=10^(-1:2), gamma=c(.5,1,2)))
model <- svm(thegroup~., data = trainset, kernel='linear', cost = 1, gamma = 1)
prediction <- predict(model, testset[,-3])
prediction
testset[,-3]
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
require(e1071)
require(MLmetrics)
require(dplyr)
df <- read.table('1215data.txt', sep='\t', header=T)
count = 1
for (i in df$身價){
if(i < 10000){
df$thegroup[count] = 'y'
count = count + 1
}else{
df$thegroup[count] = 'n'
count = count + 1
}
}
head(df, 4)
feature <- df[, c(8, 10, 13)]
index <- 1:nrow(df)
testindex <- sample(index, trunc(length(index)*30/100))
trainset <- feature[-testindex,]
testset <- feature[testindex,]
(tuned <- tune.svm(是否為網紅~., data = trainset, cost=10^(-1:2), gamma=c(.5,1,2)))
model <- svm(是否為網紅~., data = trainset, kernel='linear', cost = 1, gamma = 1)
prediction <- predict(model, testset[,-3])
source('~/.active-rstudio-document')
prediction
source('~/.active-rstudio-document')
testset[,-3]
testset[,3])
testset[,3]
source('~/.active-rstudio-document')
testset[,3]
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
df$thegroup
source('~/.active-rstudio-document')
df$thegroup
levels(df$thegroup)
label(df$thegroup)
labels(df$thegroup)
factor(df$thegroup))
factor(df$thegroup)
source('~/.active-rstudio-document')
F1_Score(prediction, testset[,3])
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('~/dsR/cluster/cluster.R')
source('~/dsR/cluster/cluster.R')
source('~/dsR/cluster/cluster.R')
source('~/dsR/cluster/cluster.R')
library(janeaustenr)
library(tidyverse)
library(tidytext)
tidy_books <- austen_books() %>%
unnest_tokens(word, text) %>% # convert sentence to token
group_by(book,word) %>%
mutate(freq = n()) %>% # calculate term count
arrange(desc(freq)) %>%
ungroup()
tidy_books<- tidy_books %>% # remove stopwords
anti_join(stop_words)
length(unique(tidy_books$word))
tidy_books <- tidy_books %>% # calculate total term count
group_by(word) %>%
mutate(total = n()) %>%
ungroup() #%>%
filter(total >= 10) # remove rare words
length(unique(tidy_books$word))
tdm <- table(tidy_books$book,tidy_books$word) %>%
as.data.frame.matrix() # build term-document matrix
# practice: build tdm using polis data
library(jiebaR)
df <- read_csv("comments_prep.csv")
library(jiebaR)
df <- read_csv("~/dsR/cluster/comments_prep.csv")
stopchi <- as.data.frame(read_lines('~/dsR/cluster/stopword.txt',skip = 1))
names(stopchi)<-'word'
library(datasets)
head(iris)
library(ggplot2)
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
set.seed(20) # set random seed to ensure reproducibility
irisCluster <- kmeans(iris[, 3:4], centers=3, nstart=20) # run kmeans model
irisCluster$cluster
irisCluster$cluster <- as.factor(irisCluster$cluster)
ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point()
log.iris <- log(iris[, 1:4]) # feature column
iris.species <- iris[, 5] # target column
iris.pca <- prcomp(log.iris,
center = TRUE, # standarization
scale. = TRUE)
rotated <- iris.pca$x %>% # data projected in pca space
as.data.frame()
ggplot(rotated, aes(PC1, PC2, color = iris$Species)) + geom_point()
PCACluster <- kmeans(rotated[, 1:2], 3, nstart = 20) # run kmeans model
PCACluster$cluster <- as.factor(PCACluster$cluster)
ggplot(rotated, aes(PC1, PC2, color = PCACluster$cluster)) + geom_point()
source('~/dsR/cluster/cluster.R')
library(janeaustenr)
library(tidyverse)
library(tidytext)
tidy_books <- austen_books() %>%
unnest_tokens(word, text) %>% # convert sentence to token
group_by(book,word) %>%
mutate(freq = n()) %>% # calculate term count
arrange(desc(freq)) %>%
ungroup()
tidy_books<- tidy_books %>% # remove stopwords
anti_join(stop_words)
length(unique(tidy_books$word))
tidy_books <- tidy_books %>% # calculate total term count
group_by(word) %>%
mutate(total = n()) %>%
ungroup() #%>%
filter(total >= 10) # remove rare words
length(unique(tidy_books$word))
tdm <- table(tidy_books$book,tidy_books$word) %>%
as.data.frame.matrix() # build term-document matrix
# practice: build tdm using polis data
library(jiebaR)
df <- read_csv("~/dsR/cluster/comments_prep.csv")
stopchi <- as.data.frame(read_lines('~/dsR/cluster/stopword.txt',skip = 1))
names(stopchi)<-'word'
# kmeans clustering using iris data
#https://dotblogs.com.tw/dragon229/2013/02/04/89919
library(datasets)
head(iris)
library(ggplot2)
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
set.seed(20) # set random seed to ensure reproducibility
irisCluster <- kmeans(iris[, 3:4], centers=3, nstart=20) # run kmeans model
irisCluster$cluster
irisCluster$cluster <- as.factor(irisCluster$cluster)
ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point()
#PCA
#http://setosa.io/ev/principal-component-analysis/
log.iris <- log(iris[, 1:4]) # feature column
iris.species <- iris[, 5] # target column
iris.pca <- prcomp(log.iris,
center = TRUE, # standarization
scale. = TRUE)
rotated <- iris.pca$x %>% # data projected in pca space
as.data.frame()
ggplot(rotated, aes(PC1, PC2, color = iris$Species)) + geom_point()
PCACluster <- kmeans(rotated[, 1:2], 3, nstart = 20) # run kmeans model
PCACluster$cluster <- as.factor(PCACluster$cluster)
ggplot(rotated, aes(PC1, PC2, color = PCACluster$cluster)) + geom_point()
source('~/dsR/cluster/cluster.R')
library(janeaustenr)
library(tidyverse)
library(tidytext)
austen_books()
austen_books() %>%
unnest_tokens(word, text)
austen_books() %>%
unnest_tokens(word, text) %>% # convert sentence to token
group_by(book,word)
austen_books() %>%
unnest_tokens(word, text) %>% # convert sentence to token
group_by(book,word) %>%
mutate(freq = n()) %>% # calculate term count
arrange(desc(freq))
austen_books() %>%
unnest_tokens(word, text) %>% # convert sentence to token
group_by(book,word) %>%
mutate(freq = n()) %>% # calculate term count
arrange(desc(freq)) %>%
ungroup()
tidy_books
tidy_books <- austen_books() %>%
unnest_tokens(word, text) %>% # convert sentence to token
group_by(book,word) %>%
mutate(freq = n()) %>% # calculate term count
arrange(desc(freq)) %>%
ungroup()
tidy_books
tidy_books <- tidy_books %>% # calculate total term count
group_by(word) %>%
mutate(total = n()) %>%
ungroup() %>%
filter(total >= 10)
tidy_books
tdm
library(datasets)
head(iris)
library(ggplot2)
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
View(iris)
?kmeans
irisCluster
irisCluster$cluster
irisCluster$cluster <- as.factor(irisCluster$cluster)
ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point()
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
iris[, 1:4]
log(iris[, 1:4])
iris.pca
ggplot(rotated, aes(PC1, PC2, color = iris$Species)) + geom_point()
PCACluster <- kmeans(rotated[, 1:2], 3, nstart = 20) # run kmeans model
PCACluster$cluster <- as.factor(PCACluster$cluster)
ggplot(rotated, aes(PC1, PC2, color = PCACluster$cluster)) + geom_point()
?kmeans
source('~/dsR/cluster/cluster.R')
View(df)
tdm
str(tdm)
source('~/dsR/cluster/cluster.R')
source('~/dsR/cluster/cluster.R')
segment('今天的天氣',mixSeg)
austen_books()
austen_books() %>%
unnest_tokens(word, text)
paste0(1:12, c("st", "nd", "rd", rep("th", 9))
)
past('a','c')
paset('a','c')
paste('a','c')
paste('a','c', 'g')
paste(segment('今天的天氣',mixSeg))
paste(c()'a','c', 'g')
paste(c('a','c', 'g'))
as.list(segment('今天的天氣',mixSeg))
paste(as.list(segment('今天的天氣',mixSeg)))
paste('c','v')
paste('c','v','d')
'c'
'c','v','d'
str(('c','v','d'))
paste(as.list(segment('今天的天氣',mixSeg)), collapse=' ')
paste(segment('今天的天氣',mixSeg), collapse=' ')
source('~/dsR/cluster/cluster.R')
head(df)
df$segText
source('~/dsR/cluster/cluster.R')
source('~/dsR/cluster/cluster.R')
View(df)
austen_books() %>%
unnest_tokens(word, text)
source('~/dsR/cluster/cluster.R')
source('~/dsR/cluster/cluster.R')
source('~/dsR/cluster/cluster.R')
df$segText
as.character(segment('殺害無反抗能力的孩童真的應處極刑',mixSeg))
source('~/dsR/cluster/cluster.R')
source('~/dsR/cluster/cluster.R')
comments
str(comments)
stopchi
source('~/dsR/cluster/cluster.R')
df <- read_csv("~/dsR/cluster/comments_prep.csv")
df <- df[,c(3,7)]
df$segText <- sapply(df$commentbody, function(x) paste((segment(x,mixSeg)),collapse = ' '))
comments <- df %>%
unnest_tokens(word, segText) %>% # convert sentence to token
group_by(authorid,word) %>%
mutate(freq = n()) %>% # calculate term count
arrange(desc(freq)) %>%
ungroup()
stopchi <- as.data.frame(read_lines('~/dsR/cluster/stopword.txt',skip = 1))
names(stopchi)<-'word'
comments <- comments %>% # remove stopwords
anti_join(stopchi$word)
source('~/dsR/cluster/cluster.R')
comments <- df %>%
unnest_tokens(word, segText) %>% # convert sentence to token
group_by(authorid,word) %>%
mutate(freq = n()) %>% # calculate term count
arrange(desc(freq)) %>%
ungroup()
stopchi <- as.data.frame(read_lines('~/dsR/cluster/stopword.txt',skip = 1))
names(stopchi)<-'word'
comments <- comments %>% # remove stopwords
anti_join(stopchi)
length(unique(comments$word))
comments <- comments %>% # calculate total term count
group_by(word) %>%
mutate(total = n()) %>%
ungroup() %>%
filter(total >= 10) # remove rare words
source('~/dsR/cluster/cluster.R')
str(comments_tdm)
?makeIcon
stores<- read.csv("taipei_store.csv",header = TRUE)
?makeIcon
?makeIcon()
shiny::runApp()
?makeIcon()
source('~/pecuR/shinyProject/source.R')
runApp()
runApp()
source('~/pecuR/shinyProject/source.R')
runApp()
source('~/pecuR/shinyProject/source.R')
runApp()
runApp()
knitr::opts_chunk$set(echo = FALSE)
summary(cars)
plot(pressure)
